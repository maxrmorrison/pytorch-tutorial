{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of PyTorch is to provide a replacement package for numpy when working with deep learning. This is necessary because\n",
    " - Numpy doesn't have GPU support\n",
    " - Numpy doesn't have automatic differentiation (autodiff)\n",
    " - Numpy doesn't have utilities for distributing computation across devices (e.g., train a model on 100 GPUs simultaneously)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Creation\n",
    "The common numpy operations for array creation also work in PyTorch. In PyTorch, we call these *tensors*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.empty(2, 3)\n",
    "b = torch.ones(3, 2)\n",
    "c = torch.zeros(3, 3)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing with Numpy\n",
    "Going back and forth between numpy and PyTorch is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "c = b.numpy()\n",
    "\n",
    "a == c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Math\n",
    "All of the typical mathematical operators are defined as expected. Unlike numpy, the types need to matchâ€”casting is not done implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.],\n",
      "        [4., 5.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[2, 3], [4,5]], dtype=torch.float)\n",
    "y = torch.ones(2, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "Let's build the simple neural network we derived. Note that this example is missing a lot of small details (e.g., batching, iterative updating, regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3624, 0.0351, 0.4195],\n",
      "        [0.5525, 0.7437, 0.5118],\n",
      "        [0.7857, 0.1524, 0.6084]], requires_grad=True)\n",
      "tensor([0.9750, 0.8846, 0.2824], requires_grad=True)\n",
      "Loss is: tensor([130.0210], grad_fn=<MulBackward>)\n",
      "tensor([[-0.4238, -0.2794, -1.1527],\n",
      "        [-0.1608,  0.4584, -0.9147],\n",
      "        [ 0.5580,  0.0613,  0.1529]], requires_grad=True)\n",
      "tensor([-0.0050, -0.6261, -1.3813], requires_grad=True)\n",
      "tensor(-6.1360, grad_fn=<DotBackward>)\n",
      "Loss is: tensor([25.4610], grad_fn=<MulBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5., 2., 10.])\n",
    "y = torch.tensor([1.])\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "theta_1 = torch.rand(3, 3, requires_grad=True)\n",
    "theta_2 = torch.rand(3, requires_grad=True)\n",
    "\n",
    "print(theta_1)\n",
    "print(theta_2)\n",
    "\n",
    "z = torch.mv(theta_1, x)\n",
    "h = torch.max(z, torch.zeros_like(z))\n",
    "y_hat = torch.dot(theta_2, h)\n",
    "loss = 0.5 * (y - y_hat)**2\n",
    "\n",
    "print('Loss is: {}'.format(loss))\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    theta_1 -= learning_rate * theta_1.grad\n",
    "    theta_2 -= learning_rate * theta_2.grad\n",
    "\n",
    "print(theta_1)\n",
    "print(theta_2)\n",
    "\n",
    "z = torch.mv(theta_1, x)\n",
    "h = torch.max(z, torch.zeros_like(z))\n",
    "y_hat = torch.dot(theta_2, h)\n",
    "loss = 0.5 * (y - y_hat)**2\n",
    "\n",
    "print('Loss is: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "PyTorch provides a lot of convenient utilities for constructing neural networks that allow us to build our networks by combining modular network components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.dense1 = nn.Linear(784, 200)\n",
    "        self.dense2 = nn.Linear(200, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense1(x))\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 2.368236541748047\n",
      "Loss is 2.0589964389801025\n",
      "Loss is 1.7804220914840698\n",
      "Loss is 1.5231589078903198\n",
      "Loss is 1.2872402667999268\n",
      "Loss is 1.079508662223816\n",
      "Loss is 0.9041410684585571\n",
      "Loss is 0.7614465951919556\n",
      "Loss is 0.6480257511138916\n",
      "Loss is 0.5590713024139404\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_data\n",
    "\n",
    "train_features, test_features, train_targets, test_targets = load_data('mnist-multiclass')\n",
    "\n",
    "x = torch.from_numpy(train_features.astype('float')).float()\n",
    "y = torch.from_numpy(train_targets.astype('float')).long()\n",
    "\n",
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Loss is {}'.format(loss.detach().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
